{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4243993-9207-4885-9c37-1d8a855c5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c0cef-0a0f-4e10-9225-154f084c666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"..\",\"..\"))\n",
    "from discotec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7855a1-96c5-4db7-b43a-6f90f45df9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "#mpl.use(\"pgf\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use('science')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e576f-a5f7-42bd-b8a4-0c1112950255",
   "metadata": {},
   "source": [
    "# Generating partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9411f-9d55-4edd-b344-0331e9d968e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[1, 2])\n",
    "def generate_reference_partition(random_key, n_samples, n_clusters):\n",
    "    y_true = jax.random.choice(random_key, n_clusters, shape=(n_samples,))\n",
    "    return y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56aa13-ac37-4ad4-b5ae-adf5c48450cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[3])\n",
    "def generate_fixedK_partition(key, reference, conservation_prob, n_clusters):\n",
    "    relabelling_key, new_cluster_key = jax.random.split(key, 2)\n",
    "    to_conserve = jax.random.bernoulli(relabelling_key, p=conservation_prob, shape=reference.shape)\n",
    "    # To keep the accuracy between expected bounds, we make sure that\n",
    "    # new_clusters is always different from the reference partition\n",
    "    new_clusters = jax.random.choice(new_cluster_key, n_clusters-1, shape=reference.shape)\n",
    "    new_clusters = (new_clusters+reference+1)%n_clusters\n",
    "    \n",
    "\n",
    "    return reference*to_conserve+new_clusters*(1-to_conserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299466a5-4127-4092-9618-c47539a14a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[1,2,3,4,5])\n",
    "def generate_scenario(random_key, n_samples, n_models, n_clusters, min_accuracy, max_accuracy):\n",
    "    reference_key, switch_key, models_key = jax.random.split(random_key, 3)\n",
    "\n",
    "    # We start by generate the labels of this scenario\n",
    "    y_true = generate_reference_partition(reference_key, n_samples, n_clusters)\n",
    "\n",
    "    if min_accuracy==max_accuracy:\n",
    "        conservation_probs = jnp.ones(n_models)*(1-min_accuracy)\n",
    "    else:\n",
    "        conservation_probs = jax.random.uniform(switch_key, minval=min_accuracy, maxval=max_accuracy, shape=(n_models,))\n",
    "\n",
    "    model_sampler = jax.vmap(generate_fixedK_partition, in_axes=[0, None, 0, None])\n",
    "    models_keys = jax.random.split(models_key, n_models)\n",
    "    y_pred = model_sampler(models_keys, y_true, conservation_probs, n_clusters)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e3b88-9f00-4de0-879d-5a51295503c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_accuracy(y_true,y_pred):\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    r, c = linear_sum_assignment(confusion_matrix, maximize=True)\n",
    "\n",
    "    return confusion_matrix[r,c].sum()/confusion_matrix.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6308e7-33e9-47be-812d-7272d40d3a53",
   "metadata": {},
   "source": [
    "# Run the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a66c5a-396f-4ffc-ae14-7f4ca4e7ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "n_models = 5\n",
    "n_runs = 50\n",
    "n_clusters = 10\n",
    "max_accuracies = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "min_accuracy = 0.1\n",
    "filename =  f\"bounded_accuracy_results_{n_models}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75561f43-7a76-44b1-b5a9-f527dbc518c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_key = jax.random.key(0)\n",
    "all_scores = []\n",
    "for max_accuracy in max_accuracies:\n",
    "    print(max_accuracy)\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "    for i in range(n_runs):\n",
    "        print(i, end=\" \")\n",
    "        # Split the random key\n",
    "        master_key, dataset_key = jax.random.split(master_key)\n",
    "    \n",
    "        # Generate the scenario\n",
    "        y_true, y_pred = generate_scenario(dataset_key, n_samples, n_models, n_clusters, min_accuracy, max_accuracy)\n",
    "    \n",
    "        # Compute the consensus matrix\n",
    "        centroid = compute_consensus_matrix(y_pred)\n",
    "    \n",
    "        # Evaluate all metrics\n",
    "        ## External validity index\n",
    "        true_aris = jnp.array([metrics.adjusted_rand_score(y_true, y) for y in y_pred])\n",
    "    \n",
    "        # Connectivity based index\n",
    "        ## Notice that we negate to get a metric to maximise (instead of minimising)\n",
    "        tv_ranking_scores = -compute_tv_ranking(y_pred, centroid)\n",
    "        hellinger_ranking_scores = -compute_hellinger_ranking(y_pred, centroid)\n",
    "        kl_ranking_scores = -compute_kl_ranking(y_pred, centroid)\n",
    "        weighted_tv_scores = -compute_weighted_tv(y_pred, centroid)\n",
    "        weighted_hellinger_scores = -compute_weighted_hellinger(y_pred, centroid)\n",
    "        weighted_kl_scores = -compute_weighted_kl(y_pred, centroid)\n",
    "\n",
    "        quantised_centroid = (centroid>centroid.mean()).astype(float)\n",
    "        quantised_scores = -compute_tv_ranking(y_pred, quantised_centroid)\n",
    "        weighted_quantised = -compute_weighted_tv(y_pred, quantised_centroid)\n",
    "        \n",
    "        pairwise_ari_scores = pairwise_score(y_pred)\n",
    "        pairwise_nmi_scores = pairwise_score(y_pred, method=\"nmi\")\n",
    "    \n",
    "        for name, scores in zip([\"DISCO_TV\",\"DISCO_WTV\",\"DISCO_KL\",\"DISCO_WKL\",\"DISCO_H\",\"DISCO_WH\",\"AARI\",\"ANMI\", \"DISCO_Q\", \"DISCO_WQ\"],\n",
    "                                [tv_ranking_scores, weighted_tv_scores, kl_ranking_scores, weighted_kl_scores, hellinger_ranking_scores, weighted_hellinger_scores,\n",
    "                                pairwise_ari_scores, pairwise_nmi_scores, quantised_scores, weighted_quantised]):\n",
    "            for corr_name, corr_fct in zip([\"Pearson\", \"Spearman\", \"Kendall\"], [stats.pearsonr, stats.spearmanr, stats.kendalltau]):\n",
    "                all_scores += [{\n",
    "                    \"Score\":name,\n",
    "                    \"Correlation\":corr_name,\n",
    "                    \"Value\":corr_fct(true_aris, scores).statistic,\n",
    "                    \"Run\":i,\n",
    "                    \"Max_acc\":max_accuracy\n",
    "                }]\n",
    "if not os.path.exists(filename):\n",
    "    df = pd.DataFrame(all_scores)\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65976e1-f14c-455b-a21b-10689505820c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_models in [5,25,50]:\n",
    "    df = pd.read_csv(f\"bounded_accuracy_results_{n_models}.csv\")\n",
    "    # For the storytelling purposes, I dropped the weighted scores\n",
    "    filtered_df = df[~df.Score.isin([\"DISCO_WQ\",\"DISCO_WH\",\"DISCO_WKL\",\"DISCO_WTV\", \"DISCO_TV\", \"DISCO_H\"])].replace({\"DISCO_TV\":\"Total variation\", \"DISCO_H\":\"Hellinger\", \"DISCO_Q\":\"Binary\", \"DISCO_KL\":\"KL\"})\n",
    "    \n",
    "    for correlation, subdf in filtered_df.groupby(\"Correlation\"):\n",
    "        axes = sns.lineplot(subdf, x=\"Max_acc\", y=\"Value\", hue=\"Score\")\n",
    "        plt.ylim((-0.2,1.1))\n",
    "        plt.ylabel(\"Ranking Correlation\")\n",
    "        plt.xlabel(r\"$\\rho_\\text{max}$\")\n",
    "        plt.savefig(f\"{correlation}_{n_models}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b4fafd-764c-4154-8aa7-b83378b49077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save as well some figures of the consensus matrices\n",
    "master_key = jax.random.key(0)\n",
    "plt.figure(figsize=(15,8))\n",
    "for i, rho in enumerate([0.2, 0.5, 0.9]):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    # Split the random key\n",
    "    master_key, dataset_key = jax.random.split(master_key)\n",
    "\n",
    "    # Generate the scenario\n",
    "    y_true, y_pred = generate_scenario(dataset_key, 200, 50, 10, 0.1, rho)\n",
    "    order = jnp.argsort(y_true)\n",
    "\n",
    "    # Compute the consensus matrix\n",
    "    centroid = compute_consensus_matrix(y_pred)\n",
    "\n",
    "    plt.imshow(centroid[order][:,order])\n",
    "    plt.title(r\"$\\rho_\\text{max}$ = \"+f\"{rho:.1f}\")\n",
    "\n",
    "    if i==0:\n",
    "        plt.ylabel(\"Raw matrix\")\n",
    "\n",
    "    plt.subplot(2,3,i+4)\n",
    "\n",
    "    quantised_centroid = (centroid>centroid.mean()).astype(float)\n",
    "\n",
    "    plt.imshow(quantised_centroid[order][:,order])\n",
    "    if i==0:\n",
    "        plt.ylabel(\"Binarised matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"example_consensus.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935e507-fc3b-4618-92a9-ddca3c134c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save as well some figures of the consensus matrices\n",
    "master_key = jax.random.key(0)\n",
    "plt.figure(figsize=(15,8))\n",
    "for i, rho in enumerate([0.2, 0.5, 0.9]):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    # Split the random key\n",
    "    master_key, dataset_key = jax.random.split(master_key)\n",
    "\n",
    "    # Generate the scenario\n",
    "    y_true, y_pred = generate_scenario(dataset_key, 200, 50, 10, 0.1, rho)\n",
    "\n",
    "    # Compute the consensus matrix\n",
    "    centroid = compute_consensus_matrix(y_pred)\n",
    "\n",
    "    # Compute the quantised score\n",
    "    quantised_centroid = (centroid>centroid.mean()).astype(float)\n",
    "\n",
    "    quantised_score = compute_tv_ranking(y_pred, quantised_centroid)\n",
    "    best_model = jnp.argmin(quantised_score)\n",
    "    order = jnp.argsort(y_pred[best_model])\n",
    "    \n",
    "\n",
    "    plt.imshow(centroid[order][:,order])\n",
    "    plt.title(r\"$\\rho_\\text{max}$ = \"+f\"{rho:.1f}\")\n",
    "\n",
    "    if i==0:\n",
    "        plt.ylabel(\"Raw matrix\")\n",
    "\n",
    "    plt.subplot(2,3,i+4)\n",
    "\n",
    "\n",
    "    plt.imshow(quantised_centroid[order][:,order])\n",
    "    if i==0:\n",
    "        plt.ylabel(\"Binarised matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"example_selection.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
