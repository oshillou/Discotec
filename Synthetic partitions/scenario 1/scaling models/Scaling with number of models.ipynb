{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4243993-9207-4885-9c37-1d8a855c5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c0cef-0a0f-4e10-9225-154f084c666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "sys.path.append(os.path.join(\"..\",\"..\"))\n",
    "from discotec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7855a1-96c5-4db7-b43a-6f90f45df9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use('science')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e576f-a5f7-42bd-b8a4-0c1112950255",
   "metadata": {},
   "source": [
    "# Generating partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9411f-9d55-4edd-b344-0331e9d968e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[1, 2])\n",
    "def generate_reference_partition(random_key, n_samples, n_clusters):\n",
    "    y_true = jax.random.choice(random_key, n_clusters, shape=(n_samples,))\n",
    "    return y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56aa13-ac37-4ad4-b5ae-adf5c48450cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[3])\n",
    "def generate_fixedK_partition(key, reference, conservation_prob, n_clusters):\n",
    "    relabelling_key, new_cluster_key = jax.random.split(key, 2)\n",
    "    to_conserve = jax.random.bernoulli(relabelling_key, p=conservation_prob, shape=reference.shape)\n",
    "    # To keep the accuracy between expected bounds, we make sure that\n",
    "    # new_clusters is always different from the reference partition\n",
    "    new_clusters = jax.random.choice(new_cluster_key, n_clusters-1, shape=reference.shape)\n",
    "    new_clusters = (new_clusters+reference+1)%n_clusters\n",
    "    \n",
    "\n",
    "    return reference*to_conserve+new_clusters*(1-to_conserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299466a5-4127-4092-9618-c47539a14a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[1,2,3,4,5])\n",
    "def generate_scenario(random_key, n_samples, n_models, n_clusters, min_accuracy, max_accuracy):\n",
    "    reference_key, switch_key, models_key = jax.random.split(random_key, 3)\n",
    "\n",
    "    # We start by generate the labels of this scenario\n",
    "    y_true = generate_reference_partition(reference_key, n_samples, n_clusters)\n",
    "\n",
    "    if min_accuracy==max_accuracy:\n",
    "        conservation_probs = jnp.ones(n_models)*(1-min_accuracy)\n",
    "    else:\n",
    "        conservation_probs = jax.random.uniform(switch_key, minval=min_accuracy, maxval=max_accuracy, shape=(n_models,))\n",
    "\n",
    "    model_sampler = jax.vmap(generate_fixedK_partition, in_axes=[0, None, 0, None])\n",
    "    models_keys = jax.random.split(models_key, n_models)\n",
    "    y_pred = model_sampler(models_keys, y_true, conservation_probs, n_clusters)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6308e7-33e9-47be-812d-7272d40d3a53",
   "metadata": {},
   "source": [
    "# Run the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a66c5a-396f-4ffc-ae14-7f4ca4e7ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "n_models = 5\n",
    "n_runs = 50\n",
    "n_clusters = 10\n",
    "max_accuracies = [0.2,0.5,0.9]\n",
    "min_accuracy = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75561f43-7a76-44b1-b5a9-f527dbc518c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "for M in jnp.arange(20, 41, step=4):\n",
    "    master_key = jax.random.key(0)\n",
    "    n_models = 5*M.item()\n",
    "    filename =  f\"bounded_accuracy_results_{n_models}.csv\"\n",
    "    for max_accuracy in max_accuracies:\n",
    "        print(max_accuracy)\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "        for i in range(n_runs):\n",
    "            print(i, end=\" \")\n",
    "            # Split the random key\n",
    "            master_key, dataset_key = jax.random.split(master_key)\n",
    "        \n",
    "            # Generate the scenario\n",
    "            y_true, y_pred = generate_scenario(dataset_key, n_samples, n_models, n_clusters, min_accuracy, max_accuracy)\n",
    "        \n",
    "            # Compute the consensus matrix\n",
    "            centroid = compute_consensus_matrix(y_pred)\n",
    "        \n",
    "            # Evaluate all metrics\n",
    "            ## External validity index\n",
    "            true_aris = jnp.array([metrics.adjusted_rand_score(y_true, y) for y in y_pred])\n",
    "        \n",
    "            # Connectivity based index\n",
    "            ## Notice that we negate to get a metric to maximise (instead of minimising)\n",
    "            tv_ranking_scores = -compute_tv_ranking(y_pred, centroid)\n",
    "            hellinger_ranking_scores = -compute_hellinger_ranking(y_pred, centroid)\n",
    "            kl_ranking_scores = -compute_kl_ranking(y_pred, centroid)\n",
    "    \n",
    "            quantised_centroid = (centroid>centroid.mean()).astype(float)\n",
    "            quantised_scores = -compute_tv_ranking(y_pred, quantised_centroid)\n",
    "            \n",
    "            pairwise_ari_scores = pairwise_score(y_pred)\n",
    "            pairwise_nmi_scores = pairwise_score(y_pred, method=\"nmi\")\n",
    "        \n",
    "            for name, scores in zip([\"DISCO_TV\",\"DISCO_KL\",\"DISCO_H\",\"AARI\",\"ANMI\", \"DISCO_Q\"],\n",
    "                                    [tv_ranking_scores, kl_ranking_scores, hellinger_ranking_scores,\n",
    "                                    pairwise_ari_scores, pairwise_nmi_scores, quantised_scores]):\n",
    "                for corr_name, corr_fct in zip([\"Pearson\", \"Spearman\", \"Kendall\"], [stats.pearsonr, stats.spearmanr, stats.kendalltau]):\n",
    "                    all_scores += [{\n",
    "                        \"Score\":name,\n",
    "                        \"Correlation\":corr_name,\n",
    "                        \"Value\":corr_fct(true_aris, scores).statistic,\n",
    "                        \"Run\":i,\n",
    "                        \"Max_acc\":max_accuracy\n",
    "                    }]\n",
    "    if not os.path.exists(filename):\n",
    "        df = pd.DataFrame(all_scores)\n",
    "        df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6a874-56f8-47d6-ae15-e8d3c42a6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataframes = []\n",
    "for filename in glob(\"*.csv\"):\n",
    "    n_models = int(filename.split(\"_\")[-1][:-4])\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    df[\"M\"] = n_models\n",
    "    all_dataframes += [df]\n",
    "df = pd.concat(all_dataframes)\n",
    "filtered_df = df[~df.Score.isin([\"DISCO_TV\", \"DISCO_H\"])].replace({\"DISCO_TV\":\"Total variation\", \"DISCO_H\":\"Hellinger\", \"DISCO_Q\":\"Binary\", \"DISCO_KL\":\"KL\"})\n",
    "filtered_df = filtered_df[(filtered_df.M%20==0)|(filtered_df.M<=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279cca5-63fa-4773-880e-2af26fbc89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_acc, subdf in filtered_df.groupby(\"Max_acc\", as_index=False):\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    for i, corr_name in enumerate([\"Pearson\", \"Spearman\", \"Kendall\"]):\n",
    "        plt.subplot(1,3,i+1)\n",
    "        sns.lineplot(data=subdf[subdf.Correlation==corr_name], x=\"M\", y=\"Value\", hue=\"Score\")\n",
    "        plt.title(\"Correlation = \"+corr_name)\n",
    "    plt.suptitle(f\"Max accuracy = {max_acc:.1%}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
