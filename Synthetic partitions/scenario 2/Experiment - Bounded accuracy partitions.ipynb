{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4243993-9207-4885-9c37-1d8a855c5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c0cef-0a0f-4e10-9225-154f084c666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(\"..\",\"..\"))\n",
    "from discotec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7855a1-96c5-4db7-b43a-6f90f45df9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use('science')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e576f-a5f7-42bd-b8a4-0c1112950255",
   "metadata": {},
   "source": [
    "# Generating partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9411f-9d55-4edd-b344-0331e9d968e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[1, 2])\n",
    "def generate_reference_partition(random_key, n_samples, n_clusters):\n",
    "    y_true = jax.random.choice(random_key, n_clusters, shape=(n_samples,))\n",
    "    return y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a56aa13-ac37-4ad4-b5ae-adf5c48450cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.vmap, in_axes=[0,None,0,None])\n",
    "@partial(jax.jit, static_argnums=[3])\n",
    "def generate_fixedK_partition(key, reference, conservation_prob, n_clusters):\n",
    "    relabelling_key, new_cluster_key = jax.random.split(key, 2)\n",
    "    to_conserve = jax.random.bernoulli(relabelling_key, p=conservation_prob, shape=reference.shape)\n",
    "    # To keep the accuracy between expected bounds, we make sure that\n",
    "    # new_clusters is always different from the reference partition\n",
    "    new_clusters = jax.random.choice(new_cluster_key, n_clusters-1, shape=reference.shape)\n",
    "    new_clusters = (new_clusters+reference+1)%n_clusters\n",
    "    \n",
    "\n",
    "    return reference*to_conserve+new_clusters*(1-to_conserve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486ffb9-ca49-4680-b901-aa0fe5750190",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[1,2,3,4])\n",
    "def generate_scenario(random_key, n_samples, n_models, n_clusters, n_ref_1):\n",
    "    reference_key, subref_key, models_key = jax.random.split(random_key, 3)\n",
    "\n",
    "    # We start by generate the labels of this scenario and all similar centroids\n",
    "    y_true = generate_reference_partition(reference_key, n_samples, n_clusters)\n",
    "\n",
    "    centroids = generate_fixedK_partition(jax.random.split(subref_key,num=2), y_true, jnp.array([0.2, 0.9]), n_clusters)\n",
    "    centroid_1, centroid_2 = centroids\n",
    "\n",
    "    models_key_1, models_key_2 = jax.random.split(models_key, num=2)\n",
    "    probs_1 = jax.random.uniform(models_key_1, shape=(n_ref_1,))\n",
    "    probs_2 = jax.random.uniform(models_key_2, shape=(n_models-n_ref_1))\n",
    "    \n",
    "    y_pred_1 = generate_fixedK_partition(jax.random.split(models_key_1, num=n_ref_1), centroid_1, probs_1, n_clusters).reshape((-1, n_samples))\n",
    "    y_pred_2 =  generate_fixedK_partition(jax.random.split(models_key_2, num=n_models-n_ref_1), centroid_2, probs_2, n_clusters).reshape((-1, n_samples))\n",
    "\n",
    "    return y_true, jnp.concatenate([y_pred_1, y_pred_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e3b88-9f00-4de0-879d-5a51295503c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_accuracy(y_true,y_pred):\n",
    "    confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "    r, c = linear_sum_assignment(confusion_matrix, maximize=True)\n",
    "\n",
    "    return confusion_matrix[r,c].sum()/confusion_matrix.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6308e7-33e9-47be-812d-7272d40d3a53",
   "metadata": {},
   "source": [
    "# Run the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a66c5a-396f-4ffc-ae14-7f4ca4e7ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "n_models = 50\n",
    "n_runs = 50\n",
    "n_clusters = 10\n",
    "n_subrefs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "filename =  f\"hierarchical_results_{n_models}.csv\"\n",
    "print(n_subrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75561f43-7a76-44b1-b5a9-f527dbc518c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_key = jax.random.key(0)\n",
    "all_correlations = []\n",
    "all_selected_aris = []\n",
    "for n_subref in n_subrefs:\n",
    "    print(n_subref)\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "    for i in range(n_runs):\n",
    "        print(i, end=\" \")\n",
    "        # Split the random key\n",
    "        master_key, dataset_key = jax.random.split(master_key)\n",
    "    \n",
    "        # Generate the scenario\n",
    "        y_true, y_pred = generate_scenario(dataset_key, n_samples, n_models, n_clusters, int(jnp.ceil(n_subref*n_models).item()))\n",
    "    \n",
    "        # Compute the consensus matrix\n",
    "        centroid = compute_consensus_matrix(y_pred)\n",
    "    \n",
    "        # Evaluate all metrics\n",
    "        ## External validity index\n",
    "        true_aris = jnp.array([metrics.adjusted_rand_score(y_true, y) for y in y_pred])\n",
    "    \n",
    "        # Connectivity based index\n",
    "        ## Notice that we negate to get a metric to maximise (instead of minimising)\n",
    "        tv_ranking_scores = -compute_tv_ranking(y_pred, centroid)\n",
    "        hellinger_ranking_scores = -compute_hellinger_ranking(y_pred, centroid)\n",
    "        kl_ranking_scores = -compute_kl_ranking(y_pred, centroid)\n",
    "\n",
    "        quantised_centroid = (centroid>centroid.mean()).astype(float)\n",
    "        quantised_scores = -compute_tv_ranking(y_pred, quantised_centroid)\n",
    "        \n",
    "        pairwise_ari_scores = pairwise_score(y_pred)\n",
    "        pairwise_nmi_scores = pairwise_score(y_pred, method=\"nmi\")\n",
    "    \n",
    "        for name, scores in zip([\"DISCO_TV\",\"DISCO_KL\",\"DISCO_H\",\"AARI\",\"ANMI\", \"DISCO_Q\"],\n",
    "                                [tv_ranking_scores, kl_ranking_scores, hellinger_ranking_scores,\n",
    "                                pairwise_ari_scores, pairwise_nmi_scores, quantised_scores]):\n",
    "            for corr_name, corr_fct in zip([\"Pearson\", \"Spearman\", \"Kendall\"], [stats.pearsonr, stats.spearmanr, stats.kendalltau]):\n",
    "                all_correlations += [{\n",
    "                    \"Score\":name,\n",
    "                    \"Correlation\":corr_name,\n",
    "                    \"Value\":corr_fct(true_aris, scores).statistic,\n",
    "                    \"Run\":i,\n",
    "                    \"Mix\":n_subref\n",
    "                }]\n",
    "            \n",
    "            all_selected_aris += [{\n",
    "                \"Score\":name,\n",
    "                \"ARI\":true_aris[jnp.argmax(scores)].item(),\n",
    "                \"Best_ARI\":max(true_aris),\n",
    "                \"Run\":i,\n",
    "                \"Mix\":n_subref\n",
    "                \n",
    "            }]\n",
    "if not os.path.exists(filename):\n",
    "    df = pd.DataFrame(all_correlations)\n",
    "    df.to_csv(filename)\n",
    "    df = pd.DataFrame(all_selected_aris)\n",
    "    df.to_csv(filename.replace(\".csv\",\"_aris.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65976e1-f14c-455b-a21b-10689505820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"hierarchical_results_50.csv\")\n",
    "# For the storytelling purposes, I dropped the weighted scores\n",
    "filtered_df = df[~df.Score.isin([\"DISCO_TV\", \"DISCO_H\"])].replace({\"DISCO_TV\":\"Total variation\", \"DISCO_H\":\"Hellinger\", \"DISCO_Q\":\"Binary\", \"DISCO_KL\":\"KL\"})\n",
    "\n",
    "for correlation, subdf in filtered_df.groupby(\"Correlation\"):\n",
    "    axes = sns.lineplot(subdf, x=\"Mix\", y=\"Value\", hue=\"Score\")\n",
    "    plt.ylim((-0.2,1.1))\n",
    "    plt.ylabel(\"Ranking Correlation\")\n",
    "    plt.xlabel(r\"$\\alpha$\")\n",
    "    plt.savefig(f\"{correlation}_{n_models}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d3a02d-0f9c-410b-bc08-ad3ac4d136ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"hierarchical_results_50_aris.csv\")\n",
    "# For the storytelling purposes, I dropped the weighted scores\n",
    "filtered_df = df[~df.Score.isin([\"DISCO_TV\", \"DISCO_H\"])].replace({\"DISCO_TV\":\"Total variation\", \"DISCO_H\":\"Hellinger\", \"DISCO_Q\":\"Binary\", \"DISCO_KL\":\"KL\"})\n",
    "\n",
    "sns.lineplot(filtered_df, x=\"Mix\", y=\"ARI\", hue=\"Score\")\n",
    "plt.ylim((-0.2,1.1))\n",
    "plt.ylabel(\"ARI\")\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.savefig(f\"ARI_{n_models}.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935e507-fc3b-4618-92a9-ddca3c134c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save as well some figures of the consensus matrices\n",
    "master_key = jax.random.key(0)\n",
    "plt.figure(figsize=(15,8))\n",
    "for i, rho in enumerate([5,25,45]):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    # Split the random key\n",
    "    master_key, dataset_key = jax.random.split(master_key)\n",
    "\n",
    "    # Generate the scenario\n",
    "    y_true, y_pred = generate_scenario(dataset_key, n_samples, 50, n_clusters, rho)\n",
    "    \n",
    "    # Compute the consensus matrix\n",
    "    centroid = compute_consensus_matrix(y_pred)\n",
    "\n",
    "    # Compute the quantised score\n",
    "    quantised_centroid = (centroid>centroid.mean()).astype(float)\n",
    "\n",
    "    quantised_score = compute_tv_ranking(y_pred, quantised_centroid)\n",
    "    best_model = jnp.argmin(quantised_score)\n",
    "    order = jnp.argsort(y_true)\n",
    "\n",
    "    print(metrics.adjusted_rand_score(y_true, y_pred[best_model]))\n",
    "    print(\"Best available ARI was:\", max([metrics.adjusted_rand_score(y_true,y) for y in y_pred]))\n",
    "    \n",
    "\n",
    "    plt.imshow(centroid[order][:,order])\n",
    "    plt.title(f\"Number of subrefs: {rho}\")\n",
    "\n",
    "    if i==0:\n",
    "        plt.ylabel(\"Raw matrix\")\n",
    "\n",
    "    plt.subplot(2,3,i+4)\n",
    "\n",
    "\n",
    "    plt.imshow(quantised_centroid[order][:,order])\n",
    "    if i==0:\n",
    "        plt.ylabel(\"Binarised matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"example_selection.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1122d80-03c5-46ba-aa4e-3074f1bf6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save as well some figures of the consensus matrices\n",
    "master_key = jax.random.key(0)\n",
    "plt.figure(figsize=(15,8))\n",
    "for i, rho in enumerate([5,25,45]):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    # Split the random key\n",
    "    master_key, dataset_key = jax.random.split(master_key)\n",
    "\n",
    "    # Generate the scenario\n",
    "    y_true, y_pred = generate_scenario(dataset_key, n_samples, 50, n_clusters, rho)\n",
    "    order = jnp.argsort(y_true)\n",
    "    \n",
    "    # Compute the consensus matrix\n",
    "    centroid = compute_consensus_matrix(y_pred)\n",
    "\n",
    "    # Compute the quantised score\n",
    "    quantised_centroid = (centroid>centroid.mean()).astype(float)\n",
    "\n",
    "    quantised_score = compute_tv_ranking(y_pred, quantised_centroid)\n",
    "    best_model = jnp.argmin(quantised_score)\n",
    "    order = jnp.argsort(y_pred[best_model])\n",
    "\n",
    "    print(metrics.adjusted_rand_score(y_true, y_pred[best_model]))\n",
    "    print(\"Best available ARI was:\", max([metrics.adjusted_rand_score(y_true,y) for y in y_pred]))\n",
    "    \n",
    "\n",
    "    plt.imshow(centroid[order][:,order])\n",
    "    plt.title(f\"Number of subrefs: {rho}\")\n",
    "\n",
    "    if i==0:\n",
    "        plt.ylabel(\"Raw matrix\")\n",
    "\n",
    "    plt.subplot(2,3,i+4)\n",
    "\n",
    "\n",
    "    plt.imshow(quantised_centroid[order][:,order])\n",
    "    if i==0:\n",
    "        plt.ylabel(\"Binarised matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"example_selection.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ebc9f-4b31-4ee5-b5fe-de2bb76313e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=[1,2,3,4])\n",
    "def generate_scenario(random_key, n_samples, n_models, n_clusters, n_ref_1):\n",
    "    reference_key, subref_key, models_key = jax.random.split(random_key, 3)\n",
    "\n",
    "    # We start by generate the labels of this scenario and all similar centroids\n",
    "    y_true = generate_reference_partition(reference_key, n_samples, n_clusters)\n",
    "\n",
    "    centroids = generate_fixedK_partition(jax.random.split(subref_key,num=2), y_true, jnp.array([0.2, 0.9]), n_clusters)\n",
    "    centroid_1, centroid_2 = centroids\n",
    "\n",
    "    models_key_1, models_key_2 = jax.random.split(models_key, num=2)\n",
    "    probs_1 = jax.random.uniform(models_key_1, shape=(n_ref_1,))\n",
    "    probs_2 = jax.random.uniform(models_key_2, shape=(n_models-n_ref_1))\n",
    "    \n",
    "    y_pred_1 = generate_fixedK_partition(jax.random.split(models_key_1, num=n_ref_1), centroid_1, probs_1, n_clusters).reshape((-1, n_samples))\n",
    "    y_pred_2 =  generate_fixedK_partition(jax.random.split(models_key_2, num=n_models-n_ref_1), centroid_2, probs_2, n_clusters).reshape((-1, n_samples))\n",
    "\n",
    "    return y_true, centroids, jnp.concatenate([y_pred_1, y_pred_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c8796-463d-436f-bb96-c3f085c29c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save as well some figures of the consensus matrices\n",
    "master_key = jax.random.key(0)\n",
    "for rho in [5,45]:\n",
    "    \n",
    "    # Split the random key\n",
    "    master_key, dataset_key = jax.random.split(master_key)\n",
    "\n",
    "    # Generate the scenario\n",
    "    y_true, centroids, y_pred = generate_scenario(dataset_key, n_samples, 50, n_clusters, rho)\n",
    "    \n",
    "    # Compute the consensus matrix\n",
    "    centroid = compute_consensus_matrix(y_pred)\n",
    "\n",
    "    # Compute the quantised score\n",
    "    quantised_centroid = (centroid>centroid.mean()).astype(float)\n",
    "\n",
    "    quantised_score = compute_tv_ranking(y_pred, quantised_centroid)\n",
    "    best_model = jnp.argmin(quantised_score)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,3))\n",
    "    for i, (name, y) in enumerate(zip([r\"$\\pi^\\star$\", r\"$\\pi^\\star_A$\", r\"$\\pi^\\star_B$\", \"Best model\"], [y_true, centroids[0], centroids[1], y_pred[best_model]])):\n",
    "        plt.subplot(1,4,i+1)\n",
    "        order = jnp.argsort(y)\n",
    "        plt.imshow(centroid[order][:,order])\n",
    "        plt.title(name)\n",
    "\n",
    "    plt.suptitle(fr\"$\\alpha =  {rho/50:.1f}$\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"example_selection_{rho}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Best available ARI was:\", max([metrics.adjusted_rand_score(y_true,y) for y in y_pred]))\n",
    "    print(\"ARI of selected model with ground truth\", metrics.adjusted_rand_score(y_true, y_pred[best_model]))\n",
    "    print(\"ARI with C1: \", metrics.adjusted_rand_score(centroids[0], y_pred[best_model]))\n",
    "    print(\"ARI with C2: \", metrics.adjusted_rand_score(centroids[1], y_pred[best_model]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
