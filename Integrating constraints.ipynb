{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df7baf-6649-4538-a16f-42d2dfdec3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from sklearn import cluster, preprocessing\n",
    "from scipy import stats\n",
    "import os\n",
    "from glob import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec66c9a-adb8-4670-8ee9-93661e64a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some important parameters\n",
    "main_folder = \"UCI datasets\"\n",
    "dataset_folder = os.path.join(main_folder, \"data\")\n",
    "result_folder = os.path.join(main_folder, \"results\")\n",
    "constraint_folder = os.path.join(main_folder, \"constraints\")\n",
    "\n",
    "\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)\n",
    "if not os.path.exists(constraint_folder):\n",
    "    os.makedirs(constraint_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670555e-5bc8-4db1-8a7d-c7dbdfa60613",
   "metadata": {},
   "source": [
    "# Step 1 - Retrieve clustering and sample constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686908c-1f53-455f-b934-98b9c695d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_file in glob(os.path.join(dataset_folder, \"*_y.csv\")):\n",
    "    master_key = jax.random.key(0)\n",
    "    \n",
    "    dataset_name = dataset_file.split(os.sep)[-1][:-6]\n",
    "\n",
    "    print(dataset_name)\n",
    "\n",
    "    if os.path.exists(os.path.join(constraint_folder, f\"{dataset_name}_regularisations.csv\")):\n",
    "        continue\n",
    "    \n",
    "    # Load targets\n",
    "    targets = pd.read_csv(dataset_file).to_numpy().reshape(-1)\n",
    "\n",
    "    # Remove datasets for which 50 constrained observations is more than half the total observations\n",
    "\n",
    "    if len(targets)/2 <= 50 :\n",
    "        continue\n",
    "    \n",
    "    all_regularisations = []\n",
    "\n",
    "    # for each clustering, sample random constraints and count violations\n",
    "    mix_run_offset = 0\n",
    "    for clustering_file in glob(os.path.join(result_folder, f\"{dataset_name}_*.pkl\")):\n",
    "        with open(clustering_file, \"rb\") as file:\n",
    "            predictions = pickle.load(file)\n",
    "        model = clustering_file.split(\"_\")[-1][:-4]\n",
    "        print(\"\\tModel\", model, end=\"\\n\\t\")\n",
    "\n",
    "        for n_constraint in range(5,51, 5):\n",
    "            for i in range(50):\n",
    "                master_key, random_key = jax.random.split(master_key)\n",
    "                selected_nodes = jax.random.choice(random_key, a=len(targets), replace=False, shape=(n_constraint,))\n",
    "                x_grid, y_grid = jnp.meshgrid(selected_nodes, selected_nodes)\n",
    "                x_grid, y_grid = x_grid.reshape(-1), y_grid.reshape(-1)\n",
    "    \n",
    "                # We must remove constraints of type (i,i)\n",
    "                different_indices = x_grid!=y_grid\n",
    "                x_grid = x_grid[different_indices]\n",
    "                y_grid = y_grid[different_indices]\n",
    "    \n",
    "                # Then, we just count how many mistakes were made\n",
    "                violations = ((predictions[:,x_grid]==predictions[:,y_grid]) != (targets[x_grid]==targets[y_grid])).mean(1)\n",
    "    \n",
    "                for j in range(len(predictions)):\n",
    "    \n",
    "                    all_regularisations += [{\n",
    "                        \"Model\":model,\n",
    "                        \"Dataset\":dataset_name,\n",
    "                        \"Run\":j,\n",
    "                        \"Run_constraints\":i,\n",
    "                        \"Regularisation\":violations[j],\n",
    "                        \"n\":n_constraint\n",
    "                    }]\n",
    "                    all_regularisations += [{\n",
    "                        \"Model\":\"mix\",\n",
    "                        \"Dataset\":dataset_name,\n",
    "                        \"Run\":j+mix_run_offset,\n",
    "                        \"Run_constraints\":i,\n",
    "                        \"Regularisation\":violations[j],\n",
    "                        \"n\":n_constraint\n",
    "                    }]\n",
    "        mix_run_offset += len(predictions)\n",
    "    pd.DataFrame(all_regularisations).to_csv(os.path.join(constraint_folder, f\"{dataset_name}_regularisations.csv\"), index=False)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b60ab4-dea1-4d76-b42c-79471511870f",
   "metadata": {},
   "source": [
    "# Report correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7b19a-033b-468c-acaf-f4e621cea706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by loading the raw scoers\n",
    "scores_df = pd.concat([pd.read_csv(x) for x in glob(os.path.join(result_folder, \"*.csv\"))], ignore_index=True)\n",
    "# Drop scores that we will not analyse\n",
    "scores_df = scores_df[~scores_df.Score.isin([\"DISCO_H\",\"DISCO_TV\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f58c8-a8b2-4d92-bcb8-b23df282713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we load the regularisations\n",
    "regularisation_df = pd.concat([pd.read_csv(x) for x in glob(os.path.join(constraint_folder, \"*.csv\"))], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de39e9-0e96-4f8a-bbff-a352037a67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We stitch together the dataframes by computing the new value (score+reg)\n",
    "# We do not perform this operation for the external metric: the ARI\n",
    "df=pd.merge(scores_df, regularisation_df, on=[\"Model\",\"Dataset\",\"Run\"], how=\"inner\")\n",
    "non_ari_scores = df.Score!=\"ARI\"\n",
    "df.loc[non_ari_scores,\"Value\"] -= df.loc[non_ari_scores, \"Regularisation\"]\n",
    "df=df.drop([\"Regularisation\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccb4b9-fe93-4191-be51-8d18b9da646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we must add entries when the scores were unconstrained\n",
    "# That means simply concatenating vertically the dataframes\n",
    "scores_df[\"n\"] = 0\n",
    "scores_df[\"Run_constraints\"] = 0 # Dummy value\n",
    "df = pd.concat([df, scores_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fc782-9e02-42ce-827a-a0031b6d31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now evaluate the correlations of this new ranking\n",
    "# We have 1 correlation value per Model/dataset/Run_constraints\n",
    "df = df.pivot(columns=\"Score\",index=[\"Dataset\",\"Model\",\"Run_constraints\", \"Run\", \"n\"], values=\"Value\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d6194-bc42-4bd3-8513-0b0e966ed87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_df = df.drop(\"Run\", axis=1).groupby([\"Dataset\",\"Model\",\"Run_constraints\",\"n\"]).corr(method=\"kendall\")\n",
    "correlations_df = correlations_df.rename(columns={\"ARI\":\"Correlation\"})[\"Correlation\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1cdb9-9477-4a3c-a47f-8f1ecba5c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, subdf in correlations_df[correlations_df.Score!=\"ARI\"].groupby(\"Model\", as_index=False):\n",
    "    print(model_name)\n",
    "    sns.lineplot(data=subdf[subdf.Dataset!=\"lung\"], x=\"n\", y=\"Correlation\", hue=\"Score\")\n",
    "    plt.xlabel(\"Constrained observations\")\n",
    "    plt.savefig(f\"regularisation_{model_name}.pdf\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
