{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ce394-0222-4b2a-a63f-6709850d6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import permetrics\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b72ac8-2ce3-4a4e-ab24-d73438ba130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"Synthetic datasets\"\n",
    "result_folder = os.path.join(root_folder, \"results\")\n",
    "data_folder = os.path.join(root_folder, \"data\")\n",
    "baseline_folder = os.path.join(root_folder, \"baseline\")\n",
    "\n",
    "\n",
    "if not os.path.exists(baseline_folder):\n",
    "    os.makedirs(baseline_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37836e4a-6cd3-4457-a46e-f6d89e26ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn_index(X, y_pred):\n",
    "    # I had to reimplement it because the permetrics library often fails to compute it\n",
    "    # Compute the distances on this dataset\n",
    "    distances = metrics.pairwise_distances(X, metric=\"euclidean\")\n",
    "    K = len(np.unique(y_pred))\n",
    "    # Now, we must identify for each pair of clusters their closest distance\n",
    "    cluster_d = np.ones((K, K))*np.inf\n",
    "    for i in range(K):\n",
    "        i_indices, = np.where(y_pred==i)\n",
    "        for j in range(i+1, K):\n",
    "            j_indices, = np.where(y_pred==j)\n",
    "            cluster_d[i,j] = distances[i_indices][:,j_indices].min()\n",
    "            cluster_d[j,i] = cluster_d[i,j]\n",
    "    \n",
    "    d_min = cluster_d.min()\n",
    "    \n",
    "    cluster_diameters = np.zeros(K)\n",
    "    for i in range(K):\n",
    "        i_indices, = np.where(y_pred==i)\n",
    "        cluster_diameters[i] = distances[i_indices][:,i_indices].max()\n",
    "    \n",
    "    d_max = cluster_diameters.max()\n",
    "    \n",
    "    return d_min/d_max\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5e03e-7ebb-4faf-b644-a48de8608277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_index(X, y_pred):\n",
    "    # C index must be minimised\n",
    "    # Compute the distances on this dataset\n",
    "    distances = metrics.pairwise_distances(X, metric=\"euclidean\")\n",
    "    \n",
    "    # Compute some constants\n",
    "    n = len(y_pred)\n",
    "    N_t = n*(n-1)//2\n",
    "    N_w = 0\n",
    "    # Compute within cluster distances\n",
    "    S_w = 0\n",
    "    for k in np.unique(y_pred):\n",
    "        indices = y_pred==k\n",
    "        S_w += distances[indices][:,indices].sum()/2 # divide by two all pairs have been seen twice\n",
    "        n_cluster = indices.sum()\n",
    "        N_w += n_cluster*(n_cluster-1)//2\n",
    "    # Extract all non-diagonal distances\n",
    "    distances = distances[np.triu_indices(len(distances),k=1)]\n",
    "    distances.sort()\n",
    "    S_min = distances[:N_w].sum()\n",
    "    S_max = distances[-N_w:].sum()\n",
    "    return (S_w-S_min)/(S_max-S_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3784d-c0c6-40bd-9862-47a42db4ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mclain_rao_index(X, y_pred):\n",
    "    # index must be minimised\n",
    "    # Compute the distances on this dataset\n",
    "    distances = metrics.pairwise_distances(X, metric=\"euclidean\")\n",
    "    \n",
    "    # Compute some constants\n",
    "    n = len(y_pred)\n",
    "    N_t = n*(n-1)//2\n",
    "    N_w = 0\n",
    "    # Compute within cluster distances, and between cluster distances\n",
    "    S_w = 0\n",
    "    S_b = 0\n",
    "    cluster_ids = np.unique(y_pred)\n",
    "    for i, k1 in enumerate(cluster_ids):\n",
    "        indices1 = y_pred==k1\n",
    "        S_w += distances[indices1][:,indices1].sum()/2 # divide by two all pairs have been seen twice\n",
    "        n_cluster = indices1.sum()\n",
    "        N_w += n_cluster*(n_cluster-1)//2\n",
    "        for k2 in cluster_ids[i+1:]:\n",
    "            indices2 = y_pred==k2\n",
    "            S_b += distances[indices1][:,indices2].sum()\n",
    "    N_b = N_t-N_w\n",
    "    S_w /= N_w\n",
    "    S_b /= N_b\n",
    "    return S_w/S_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0d291-412f-4a44-ab68-a685b08f4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pbm_index(X, y_pred):\n",
    "    data_centroid = X.mean(0, keepdims=True)\n",
    "    E_t = metrics.pairwise_distances(X, data_centroid).sum()\n",
    "    \n",
    "    # Compute some constants\n",
    "    n = len(y_pred)\n",
    "    N_t = n*(n-1)//2\n",
    "    N_w = 0\n",
    "    # Compute within cluster distances\n",
    "    E_w = 0\n",
    "    all_centroids = []\n",
    "    cluster_ids = np.unique(y_pred)\n",
    "    for k in cluster_ids:\n",
    "        indices = y_pred==k\n",
    "        cluster_centroid = X[indices].mean(0, keepdims=True)\n",
    "        all_centroids += [cluster_centroid]\n",
    "\n",
    "        E_w += metrics.pairwise_distances(X[indices], cluster_centroid).sum()\n",
    "    centroid_distances = metrics.pairwise_distances(np.concatenate(all_centroids, axis=0))\n",
    "    D = centroid_distances.max()\n",
    "\n",
    "    return np.square(E_t*D/E_w/len(cluster_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa5251-2ed6-4df0-8e41-bf42404ffe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wemmert_gancarski_index(X, y_pred):\n",
    "    cluster_ids = np.unique(y_pred)\n",
    "\n",
    "    cluster_centroids = [X[y_pred==k].mean(0, keepdims=True) for k in cluster_ids]\n",
    "    distances_to_centroids = metrics.pairwise_distances(X, np.concatenate(cluster_centroids, axis=0))\n",
    "    \n",
    "    # Compute within cluster distances\n",
    "    J = 0\n",
    "    for i, k in enumerate(cluster_ids):\n",
    "        indices = y_pred==k\n",
    "        cluster_distances = distances_to_centroids[indices]\n",
    "        R = cluster_distances[:,i] / np.delete(cluster_distances, i, axis=1).min()\n",
    "        J += max(0, 1-R.mean()) * len(indices)\n",
    "\n",
    "    return J/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a79a4c1-f8a4-4430-a7bf-5be13df68c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fcts = [x for x in dir(permetrics.ClusteringMetric) if x[-1]==\"I\"]\n",
    "smaller_is_better = [\"BHI\", \"XBI\", \"DBI\", \"BRI\", \"KDI\", \"SSEI\", \"MSEI\", \"DHI\", \"BI\", \"HI\", \"CI\", \"McRao\"]\n",
    "bigger_is_better = [\"DRI\", \"DI\", \"CHI\", \"LDRI\", \"LSRI\", \"SI\", \"RSI\", \"DBCVI\",\"PBM\", \"WG\"]\n",
    "for dataset_file in glob(os.path.join(data_folder, \"*_X.csv\")):\n",
    "    all_scores = []\n",
    "    print(dataset_file)\n",
    "    dataset_name = dataset_file.split(os.sep)[-1][:-6]\n",
    "    if os.path.exists(os.path.join(baseline_folder, f\"{dataset_name}_baseline.csv\")):\n",
    "        continue\n",
    "    X = pd.read_csv(dataset_file).to_numpy()\n",
    "    y_true = pd.read_csv(dataset_file.replace(\"_X.csv\", \"_y.csv\")).to_numpy().reshape(-1)\n",
    "\n",
    "    for prediction_file in glob(os.path.join(result_folder, dataset_name+\"_*.pkl\")):\n",
    "        model_name = prediction_file.split(\"_\")[-1][:-4]\n",
    "        print(f\"\\tModel = {model_name}\")\n",
    "        with open(prediction_file, \"rb\") as file:\n",
    "            predictions = pickle.load(file)\n",
    "\n",
    "        for i, y_pred in enumerate(predictions):\n",
    "            y_pred = preprocessing.LabelEncoder().fit_transform(y_pred) # This tackles DBSCAN -1 cluster\n",
    "            evaluator = permetrics.ClusteringMetric(y_pred=y_pred.tolist(), X=X)\n",
    "            for score_fct in score_fcts:\n",
    "                try:\n",
    "                    if score_fct not in [\"DI\", \"CI\", \"McRao\", \"PBM\", \"WG\"]:\n",
    "                        value = evaluator.__getattribute__(score_fct)()\n",
    "                    elif score_fct==\"DI\":\n",
    "                        value = dunn_index(X, y_pred)\n",
    "                    elif score_fct==\"WG\":\n",
    "                        value = wemmert_gancarski_index(X, y_pred)\n",
    "                    elif score_fct == \"PBM\":\n",
    "                        value = pbm_index(X, y_pred)\n",
    "                    elif score_fct==\"CI\":\n",
    "                        value = c_index(X, y_pred)\n",
    "                except:\n",
    "                    value = np.nan\n",
    "                if score_fct in smaller_is_better:\n",
    "                    value = -value # To leverage a positive correlation when the metric goes on the right track\n",
    "                all_scores += [{\n",
    "                    \"Dataset\":dataset_name,\n",
    "                    \"Model\":model_name,\n",
    "                    \"Run\":i,\n",
    "                    \"Score\":score_fct,\n",
    "                    \"Value\":value\n",
    "                }]\n",
    "            all_scores += [{\n",
    "                \"Dataset\":dataset_name,\n",
    "                \"Model\":model_name,\n",
    "                \"Run\":i,\n",
    "                \"Score\":\"ARI\",\n",
    "                \"Value\":metrics.adjusted_rand_score(y_true, y_pred)\n",
    "            }]\n",
    "    pd.DataFrame(all_scores).to_csv(os.path.join(baseline_folder, f\"{dataset_name}_baseline.csv\"), index=False)\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
