{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df7baf-6649-4538-a16f-42d2dfdec3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from discotec import *\n",
    "import numpy as onp\n",
    "from sklearn import cluster, preprocessing\n",
    "from scipy import stats\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec66c9a-adb8-4670-8ee9-93661e64a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some important parameters\n",
    "dataset_folder = os.path.join(\"..\",\"fcps_data\")\n",
    "result_folder = \"results\"\n",
    "\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670555e-5bc8-4db1-8a7d-c7dbdfa60613",
   "metadata": {},
   "source": [
    "# Step 1 - Perform clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376027b-592a-4075-b9de-fe04d748cacb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset in glob(os.path.join(dataset_folder, \"*_X.csv\")):\n",
    "    dataset_name = dataset.split(os.sep)[-1][:-6]\n",
    "    print(dataset_name)\n",
    "    \n",
    "    X = pd.read_csv(dataset).to_numpy()\n",
    "    \n",
    "    # Preprocess it\n",
    "    X_scaled = preprocessing.StandardScaler().fit_transform(X)\n",
    "\n",
    "    filename = os.path.join(result_folder, f\"{dataset_name}_kmeans.pkl\")\n",
    "    if not os.path.exists(filename):\n",
    "        predictions = []\n",
    "        for k in range(2, 21):\n",
    "            for _ in range(5):\n",
    "                model = cluster.KMeans(n_clusters=k)\n",
    "                y_pred = model.fit_predict(X_scaled).reshape((1,-1))\n",
    "                predictions += [y_pred]\n",
    "        predictions = jnp.concatenate(predictions, axis=0)\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(predictions, file)\n",
    "            \n",
    "    filename = os.path.join(result_folder, f\"{dataset_name}_dbscan.pkl\")\n",
    "    if not os.path.exists(filename):\n",
    "        predictions = []\n",
    "        distances = metrics.pairwise_distances(X_scaled)\n",
    "        distances = distances[jnp.triu_indices(len(distances), k=1)]\n",
    "        min_d, max_d = jnp.quantile(distances, q=0.01), jnp.quantile(distances, q=0.25)\n",
    "        for eps in jnp.linspace(min_d, max_d, num=20):\n",
    "            for _ in range(5):\n",
    "                model = cluster.DBSCAN(eps=eps.item())\n",
    "                y_pred = model.fit_predict(X_scaled).reshape((1,-1))\n",
    "                if len(jnp.unique(y_pred))>1: # Refuse degenerate solutions\n",
    "                    predictions += [y_pred]\n",
    "        \n",
    "        predictions = jnp.concatenate(predictions, axis=0)\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(predictions, file)\n",
    "\n",
    "\n",
    "    filename = os.path.join(result_folder, f\"{dataset_name}_agglomerative.pkl\")\n",
    "    if not os.path.exists(filename):\n",
    "        predictions = []\n",
    "        for k in range(2, 21):\n",
    "            for linkage in [\"single\", \"complete\", \"ward\", \"average\"]:\n",
    "                model = cluster.AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "                y_pred = model.fit_predict(X_scaled).reshape((1,-1))\n",
    "                predictions += [y_pred]\n",
    "        predictions = jnp.concatenate(predictions, axis=0)\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015942c-9bdd-4979-b444-6e0c078fae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_file in glob(os.path.join(dataset_folder, \"*_X.csv\")):\n",
    "    dataset_name = dataset_file.split(os.sep)[-1][:-6]\n",
    "    plt.figure(figsize=(15,5))\n",
    "\n",
    "    targets = pd.read_csv(dataset_file.replace(\"_X.csv\", \"_y.csv\")).to_numpy().reshape((-1))\n",
    "    order = jnp.argsort(targets)\n",
    "\n",
    "    cumulated_consensus = jnp.zeros((len(targets), len(targets)))\n",
    "    total_models = 0\n",
    "\n",
    "    for i, filename in enumerate(glob(os.path.join(result_folder, dataset_name+\"_*.pkl\"))):\n",
    "        plt.subplot(1,4,i+1)\n",
    "\n",
    "        with open(filename, \"rb\") as file:\n",
    "            predictions = pickle.load(file)\n",
    "\n",
    "        model_name = filename.split(\"_\")[-1][:-4] # Remove pkl extension\n",
    "\n",
    "        C = compute_consensus_matrix(predictions)\n",
    "        cumulated_consensus += C*len(predictions)\n",
    "        total_models += len(predictions)\n",
    "\n",
    "        plt.imshow(C[order][:,order])\n",
    "        plt.title(f\"{model_name} ({len(predictions)} models)\")\n",
    "\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow((cumulated_consensus/total_models)[order][:,order])\n",
    "    plt.title(f\"Mix ({total_models} models)\")\n",
    "    plt.suptitle(dataset_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda44cd-2c60-42fb-87e1-991ad1eba634",
   "metadata": {},
   "source": [
    "# Evaluate the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0d084-b376-4a80-8a28-3457f82d107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_file in glob(os.path.join(dataset_folder, \"*_X.csv\")):\n",
    "    dataset_name = dataset_file.split(os.sep)[-1][:-6]\n",
    "    result_filename = os.path.join(result_folder, f\"{dataset_name}_scores.csv\")\n",
    "    print(dataset_name)\n",
    "\n",
    "    if os.path.exists(result_filename):\n",
    "        continue\n",
    "\n",
    "    all_scores = []\n",
    "    y_true = pd.read_csv(dataset_file.replace(\"_X.csv\", \"_y.csv\")).to_numpy().reshape((-1))    # Get the targets\n",
    "\n",
    "    for model in [\"kmeans\",\"dbscan\",\"agglomerative\", \"mix\"]:\n",
    "        print(\"Model\", model)\n",
    "\n",
    "        if model == \"mix\":\n",
    "            predictions = []\n",
    "            for filename in glob(os.path.join(result_folder, dataset_name+\"_*.pkl\")):\n",
    "                with open(filename, \"rb\") as file:\n",
    "                    predictions += [pickle.load(file)]\n",
    "            predictions = jnp.concatenate(predictions, axis=0)\n",
    "        else:\n",
    "            with open(os.path.join(result_folder, f\"{dataset_name}_{model}.pkl\"), \"rb\") as file:\n",
    "                predictions = pickle.load(file)           \n",
    "    \n",
    "    \n",
    "        # Compute the ARI of the clusterings\n",
    "        print(\"\\tComputing ARI scores\")\n",
    "        ari_scores = [metrics.adjusted_rand_score(y_true, y) for y in predictions]\n",
    "    \n",
    "        # Compute all discotec scores\n",
    "        print(\"\\tComputing discotec scores\")\n",
    "        consensus = compute_consensus_matrix(predictions)\n",
    "        quantised_consensus = (consensus>consensus.mean()).astype(float)\n",
    "        \n",
    "        discotec_tv = -compute_tv_ranking(predictions, consensus)\n",
    "        discotec_kl = -compute_kl_ranking(predictions, consensus)\n",
    "        discotec_hellinger = -compute_hellinger_ranking(predictions, consensus)\n",
    "    \n",
    "        discotec_quantised = -compute_tv_ranking(predictions, quantised_consensus)\n",
    "        \n",
    "        print(\"\\tComputing pairwise scores\")\n",
    "        pairwise_ari_scores = pairwise_score(onp.array(predictions))\n",
    "        pairwise_nmi_scores = pairwise_score(onp.array(predictions), method=\"nmi\")\n",
    "    \n",
    "        print(\"\\tStoring results\")\n",
    "        for name, scores in zip([\"DISCO_TV\", \"DISCO_KL\", \"DISCO_H\", \"DISCO_Q\", \"AARI\", \"ANMI\", \"ARI\"],\n",
    "                               [discotec_tv, discotec_kl, discotec_hellinger, discotec_quantised,\n",
    "                                pairwise_ari_scores, pairwise_nmi_scores, ari_scores]):\n",
    "            for i in range(len(scores)):\n",
    "                all_scores += [{\n",
    "                    \"Model\":model,\n",
    "                    \"Run\":i,\n",
    "                    \"Score\": name,\n",
    "                    \"Dataset\":dataset_name,\n",
    "                    \"Value\":scores[i]\n",
    "                }]\n",
    "        \n",
    "    pd.DataFrame(all_scores).to_csv(result_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
