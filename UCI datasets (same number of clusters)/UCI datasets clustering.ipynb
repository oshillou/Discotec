{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df7baf-6649-4538-a16f-42d2dfdec3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from discotec import *\n",
    "import uci_data\n",
    "from sklearn import cluster, preprocessing\n",
    "from scipy import stats\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec66c9a-adb8-4670-8ee9-93661e64a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some important parameters\n",
    "dataset_folder = \"data\"\n",
    "result_folder = \"results\"\n",
    "\n",
    "\n",
    "if not os.path.exists(result_folder):\n",
    "    os.makedirs(result_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670555e-5bc8-4db1-8a7d-c7dbdfa60613",
   "metadata": {},
   "source": [
    "# Step 1 - Perform clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376027b-592a-4075-b9de-fe04d748cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_loader in [fct for fct in dir(uci_data) if \"load_\" in fct]:\n",
    "    dataset_name = dataset_loader[5:]\n",
    "    print(dataset_name)\n",
    "\n",
    "    dataset_file = os.path.join(dataset_folder, dataset_name+\"_X.csv\")\n",
    "    if not os.path.exists(dataset_file):\n",
    "        print(f\"Retrieving {dataset_name}\")\n",
    "        X, y = getattr(uci_data, dataset_loader)()\n",
    "        pd.DataFrame(X).to_csv(dataset_file, index=False)\n",
    "        pd.DataFrame(y).to_csv(dataset_file.replace(\"X.csv\",\"y.csv\"), index=False)\n",
    "    else:\n",
    "        print(f\"Loading {dataset_name}\")\n",
    "        X = pd.read_csv(dataset_file).to_numpy()\n",
    "        y = pd.read_csv(dataset_file.replace(\"X.csv\",\"y.csv\")).to_numpy().reshape(-1)\n",
    "\n",
    "    K = len(jnp.unique(y))\n",
    "\n",
    "    filename = os.path.join(result_folder, f\"{dataset_name}_kmeans.pkl\")\n",
    "    if not os.path.exists(filename):\n",
    "        predictions = []\n",
    "        for _ in range(50):\n",
    "            model = cluster.KMeans(n_clusters=K)\n",
    "            y_pred = model.fit_predict(X).reshape((1,-1))\n",
    "            predictions += [y_pred]\n",
    "        predictions = jnp.concatenate(predictions, axis=0)\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(predictions, file)\n",
    "            \n",
    "\n",
    "\n",
    "    filename = os.path.join(result_folder, f\"{dataset_name}_agglomerative.pkl\")\n",
    "    if not os.path.exists(filename):\n",
    "        predictions = []\n",
    "        for linkage in [\"single\", \"complete\", \"ward\", \"average\"]:\n",
    "            for metric in [\"euclidean\", \"manhattan\"]:\n",
    "                if metric==\"manhattan\" and linkage==\"ward\":\n",
    "                    continue\n",
    "                model = cluster.AgglomerativeClustering(n_clusters=K, linkage=linkage, metric=metric)\n",
    "            y_pred = model.fit_predict(X).reshape((1,-1))\n",
    "            predictions += [y_pred]\n",
    "        predictions = jnp.concatenate(predictions, axis=0)\n",
    "        with open(filename, \"wb\") as file:\n",
    "            pickle.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4f142-dbed-4a75-bf69-be3489add9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i, dataset_file in enumerate(glob(os.path.join(dataset_folder, \"*_X.csv\"))):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    dataset_name = dataset_file.split(os.sep)[-1][:-6]\n",
    "\n",
    "    targets = pd.read_csv(dataset_file.replace(\"_X.csv\", \"_y.csv\")).to_numpy().reshape((-1))\n",
    "    order = jnp.argsort(targets)\n",
    "\n",
    "    with open(os.path.join(result_folder, f\"{dataset_name}_kmeans.pkl\"), \"rb\") as file:\n",
    "        predictions = pickle.load(file)\n",
    "\n",
    "    C = compute_consensus_matrix(predictions)\n",
    "\n",
    "    plt.imshow(C[order][:,order])\n",
    "    plt.title(f\"{dataset_name} models\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8053cda-d276-41d9-8255-edb20ddb07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i, dataset_file in enumerate(glob(os.path.join(dataset_folder, \"*_X.csv\"))):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    dataset_name = dataset_file.split(os.sep)[-1][:-6]\n",
    "\n",
    "    targets = pd.read_csv(dataset_file.replace(\"_X.csv\", \"_y.csv\")).to_numpy().reshape((-1))\n",
    "    order = jnp.argsort(targets)\n",
    "\n",
    "    with open(os.path.join(result_folder, f\"{dataset_name}_agglomerative.pkl\"), \"rb\") as file:\n",
    "        predictions = pickle.load(file)\n",
    "\n",
    "    C = compute_consensus_matrix(predictions)\n",
    "\n",
    "    plt.imshow(C[order][:,order])\n",
    "    plt.title(f\"{dataset_name} models\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda44cd-2c60-42fb-87e1-991ad1eba634",
   "metadata": {},
   "source": [
    "# Evaluate the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0d084-b376-4a80-8a28-3457f82d107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_file in glob(os.path.join(dataset_folder, \"*_X.csv\")):\n",
    "    dataset_name = dataset_file.split(os.sep)[-1][:-6]\n",
    "    result_filename = os.path.join(result_folder, f\"{dataset_name}_scores.csv\")\n",
    "    print(dataset_name)\n",
    "\n",
    "    if os.path.exists(result_filename):\n",
    "        continue\n",
    "\n",
    "    all_scores = []\n",
    "    y_true = pd.read_csv(dataset_file.replace(\"_X.csv\", \"_y.csv\")).to_numpy().reshape((-1))    # Get the targets\n",
    "\n",
    "    for model in [\"kmeans\",\"agglomerative\"]:\n",
    "        print(\"Model\", model)\n",
    "\n",
    "        with open(os.path.join(result_folder, f\"{dataset_name}_{model}.pkl\"), \"rb\") as file:\n",
    "            predictions = pickle.load(file)           \n",
    "    \n",
    "    \n",
    "        # Compute the ARI of the clusterings\n",
    "        print(\"\\tComputing ARI scores\")\n",
    "        ari_scores = [metrics.adjusted_rand_score(y_true, y) for y in predictions]\n",
    "    \n",
    "        # Compute all discotec scores\n",
    "        print(\"\\tComputing discotec scores\")\n",
    "        consensus = compute_consensus_matrix(predictions)\n",
    "        quantised_consensus = (consensus>consensus.mean()).astype(float)\n",
    "        \n",
    "        discotec_tv = -compute_tv_ranking(predictions, consensus)\n",
    "        discotec_kl = -compute_kl_ranking(predictions, consensus)\n",
    "        discotec_hellinger = -compute_hellinger_ranking(predictions, consensus)\n",
    "    \n",
    "        discotec_quantised = -compute_tv_ranking(predictions, quantised_consensus)\n",
    "        \n",
    "        print(\"\\tComputing pairwise scores\")\n",
    "        pairwise_ari_scores = pairwise_score(predictions)\n",
    "        pairwise_nmi_scores = pairwise_score(predictions, method=\"nmi\")\n",
    "    \n",
    "        print(\"\\tStoring results\")\n",
    "        for name, scores in zip([\"DISCO_TV\", \"DISCO_KL\", \"DISCO_H\", \"DISCO_Q\", \"AARI\", \"ANMI\", \"ARI\"],\n",
    "                               [discotec_tv, discotec_kl, discotec_hellinger, discotec_quantised,\n",
    "                                pairwise_ari_scores, pairwise_nmi_scores, ari_scores]):\n",
    "            for i in range(len(scores)):\n",
    "                all_scores += [{\n",
    "                    \"Model\":model,\n",
    "                    \"Run\":i,\n",
    "                    \"Score\": name,\n",
    "                    \"Dataset\":dataset_name,\n",
    "                    \"Value\":scores[i]\n",
    "                }]\n",
    "        \n",
    "    pd.DataFrame(all_scores).to_csv(result_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
